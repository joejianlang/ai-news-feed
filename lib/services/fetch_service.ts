
import { createClient } from '@supabase/supabase-js';
import { getActiveNewsSources, updateLastFetchedTime } from '@/lib/supabase/queries';
import { scrapeContent } from '@/lib/scrapers';
import { analyzeContent, AnalysisResult } from '@/lib/ai';

// Initialize Service Role Client for admin operations
const serviceRoleKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
if (!serviceRoleKey) {
    console.error('âŒ CRITICAL ERROR: Missing SUPABASE_SERVICE_ROLE_KEY');
}

const supabaseAdmin = createClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL || process.env.SUPABASE_URL!,
    serviceRoleKey || process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
);

export async function updateFetchStatus(status: any) {
    try {
        await supabaseAdmin
            .from('system_settings')
            .upsert({
                key: 'fetch_status',
                value: status,
                updated_at: new Date().toISOString()
            }, { onConflict: 'key' });
    } catch (error) {
        console.error('Failed to update fetch status:', error);
    }
}

/**
 * Shuffle array using Fisher-Yates algorithm
 */
function shuffleArray<T>(array: T[]): T[] {
    const newArray = [...array];
    for (let i = newArray.length - 1; i > 0; i--) {
        const j = Math.floor(Math.random() * (i + 1));
        [newArray[i], newArray[j]] = [newArray[j], newArray[i]];
    }
    return newArray;
}

export async function runFetchPipeline(specificSourceId?: string) {
    try {
        const sources = await getActiveNewsSources();
        const targetSources = specificSourceId
            ? sources.filter(s => s.id === specificSourceId)
            : sources;

        // 0. Pre-fetch categories for mapping
        const { data: categoriesData } = await supabaseAdmin.from('categories').select('id, name');
        const categoryMap: Record<string, string> = {};
        categoriesData?.forEach(c => categoryMap[c.name] = c.id);

        const batchId = crypto.randomUUID();
        const batchTime = new Date().toISOString();

        // Initialize status
        await updateFetchStatus({
            is_running: true,
            current_source: 'é˜¶æ®µ 1: æ­£åœ¨ä»å„æºæŠ“å–åŸå§‹æ•°æ®...',
            progress: 0,
            total: targetSources.length,
            started_at: batchTime
        });

        console.log(`ğŸš€ å¼€å§‹æŠ“å–é˜¶æ®µ: å…± ${targetSources.length} ä¸ªæº`);

        // --- STAGE 1: å¿«é€ŸæŠ“å–å¹¶å­˜ä¸ºè‰ç¨¿ ---
        let totalScraped = 0;
        for (let i = 0; i < targetSources.length; i++) {
            const source = targetSources[i];
            await updateFetchStatus({ is_running: true, current_source: `æŠ“å–ä¸­: ${source.name}`, progress: i, total: targetSources.length });

            try {
                const scrapedItems = await scrapeContent(source.url, source.source_type, source.youtube_channel_id);

                for (const item of scrapedItems) {
                    // æ£€æŸ¥å»é‡
                    const { data: exists } = await supabaseAdmin.rpc('find_similar_news', {
                        check_title: item.title,
                        check_url: item.url,
                        time_window_hours: 48,
                        similarity_threshold: 0.8
                    });

                    if (exists && exists.length > 0) continue;

                    // å­˜ä¸ºè‰ç¨¿
                    await supabaseAdmin.from('news_items').insert([{
                        source_id: source.id,
                        original_url: item.url,
                        title: item.title,
                        content: item.content,
                        content_type: item.contentType,
                        published_at: item.publishedAt?.toISOString(),
                        video_id: item.videoId,
                        image_url: item.imageUrl,
                        fetch_batch_id: batchId,
                        is_published: false, // åˆå§‹ä¸ºè‰ç¨¿
                    }]);
                    totalScraped++;
                }

                await updateLastFetchedTime(source.id);
            } catch (err) {
                console.error(`Failed to scrape ${source.name}:`, err);
            }
        }

        console.log(`âœ… é˜¶æ®µ 1 å®Œæˆ: æŠ“å–åˆ° ${totalScraped} æ¡æ–°å†…å®¹ã€‚å‡†å¤‡è¿›å…¥é˜¶æ®µ 2 æ··åˆå¤„ç†...`);

        // --- STAGE 2: ä¼˜å…ˆå¤„ç†ç§¯å‹çš„æ—§è‰ç¨¿ ---
        // è·å–æ‰€æœ‰æœªå‘å¸ƒçš„æ¡ç›®ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼ˆæœ€è€çš„ä¼˜å…ˆï¼‰ï¼Œä»¥æ¸…ç†ç§¯å‹
        const { data: drafts } = await supabaseAdmin
            .from('news_items')
            .select('*, source:news_sources(commentary_style)')
            .eq('is_published', false)
            .order('created_at', { ascending: true })
            .limit(200); // å¢åŠ æ‰¹æ¬¡å¤§å°

        if (!drafts || drafts.length === 0) {
            await updateFetchStatus({ is_running: false, current_source: 'æ— æ–°å†…å®¹éœ€å¤„ç†', progress: targetSources.length, total: targetSources.length });
            return { success: true, newItems: 0 };
        }

        console.log(`ğŸ§  å¼€å§‹ AI å¤„ç†é˜¶æ®µ: å¾…å¤„ç† ${drafts.length} æ¡æ–°é—» (ä¼˜å…ˆå¤„ç†æœ€æ—©å…¥åº“çš„å†…å®¹)`);

        let successCount = 0;
        for (let i = 0; i < drafts.length; i++) {
            const news = drafts[i];

            await updateFetchStatus({
                is_running: true,
                current_source: `AI åˆ†æä¸­ (${i + 1}/${drafts.length}): ${news.title.substring(0, 20)}...`,
                progress: i,
                total: drafts.length
            });

            try {
                // è°ƒç”¨åˆå¹¶åçš„ AI æ¥å£ï¼ˆåŒ…å«ç¿»è¯‘ã€æ‘˜è¦ã€è¯„è®ºã€åˆ†ç±»ã€æ ‡ç­¾ã€åœ°ç‚¹ï¼‰
                const analysis = await analyzeContent(
                    news.content,
                    news.title,
                    news.source?.commentary_style || '',
                    news.content_type || 'article'
                );

                if (analysis.shouldSkip) {
                    await supabaseAdmin.from('news_items').delete().eq('id', news.id);
                    continue;
                }

                // é²æ£’çš„åˆ†ç±»æ˜ å°„
                let categoryName = analysis.category || 'çƒ­ç‚¹';
                // å¸¸è§çš„ AI å˜ä½“å¤„ç†
                if (categoryName.includes('æœ¬åœ°')) categoryName = 'æœ¬åœ°';
                else if (categoryName.includes('çƒ­ç‚¹')) categoryName = 'çƒ­ç‚¹';
                else if (categoryName.includes('ç§‘æŠ€')) categoryName = 'ç§‘æŠ€';
                else if (categoryName.includes('è´¢ç»')) categoryName = 'è´¢ç»';
                else if (categoryName.includes('æ·±åº¦')) categoryName = 'æ·±åº¦';

                const catId = categoryMap[categoryName] || categoryMap['çƒ­ç‚¹'] || Object.values(categoryMap)[0];

                console.log(`[AIç»“æœ] æ ‡é¢˜: ${analysis.translatedTitle?.substring(0, 20)}..., åˆ†ç±»: ${categoryName}, æ ‡ç­¾: ${JSON.stringify(analysis.tags)}`);

                // ç«‹å³æ›´æ–°å¹¶å‘å¸ƒ
                const { error: updateError } = await supabaseAdmin.from('news_items').update({
                    title: analysis.translatedTitle || news.title,
                    ai_summary: analysis.summary,
                    ai_commentary: analysis.commentary,
                    category_id: catId,
                    tags: Array.isArray(analysis.tags) ? analysis.tags : [],
                    location: analysis.location,
                    is_published: true,
                    batch_completed_at: batchTime,
                    updated_at: new Date().toISOString()
                }).eq('id', news.id);

                if (updateError) {
                    throw new Error(`æ›´æ–°æ•°æ®åº“å¤±è´¥: ${updateError.message}`);
                }

                successCount++;
                console.log(`[OK] å·²å‘å¸ƒ (${i + 1}/${drafts.length}): ${analysis.translatedTitle || news.title}`);

                // ç¨å¾®å»¶è¿Ÿï¼Œä¿æŠ¤ API
                await new Promise(r => setTimeout(r, 500));
            } catch (err) {
                console.error(`AI é˜¶æ®µå¤„ç†å¤±è´¥ [${news.id}]:`, err);
            }
        }

        // Final Status
        await updateFetchStatus({
            is_running: false,
            current_source: `å¤„ç†å®Œæˆ: æ–°å‘å¸ƒ ${successCount} æ¡`,
            progress: drafts.length,
            total: drafts.length,
            last_completed_at: new Date().toISOString()
        });

        return { success: true, newItems: successCount };

    } catch (error) {
        console.error('Error in fetch pipeline:', error);
        await updateFetchStatus({ is_running: false, error: String(error) });
        throw error;
    }
}

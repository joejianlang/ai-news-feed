
import { createClient } from '@supabase/supabase-js';
import { getActiveNewsSources, updateLastFetchedTime } from '@/lib/supabase/queries';
import { scrapeContent } from '@/lib/scrapers';
import { analyzeContent, AnalysisResult } from '@/lib/ai';

// Initialize Service Role Client for admin operations
const serviceRoleKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
if (!serviceRoleKey) {
    console.error('âŒ CRITICAL ERROR: Missing SUPABASE_SERVICE_ROLE_KEY');
}

const supabaseAdmin = createClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL || process.env.SUPABASE_URL!,
    serviceRoleKey || process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
);

export async function updateFetchStatus(status: any) {
    try {
        await supabaseAdmin
            .from('system_settings')
            .upsert({
                key: 'fetch_status',
                value: status,
                updated_at: new Date().toISOString()
            }, { onConflict: 'key' });
    } catch (error) {
        console.error('Failed to update fetch status:', error);
    }
}

/**
 * Shuffle array using Fisher-Yates algorithm
 */
function shuffleArray<T>(array: T[]): T[] {
    const newArray = [...array];
    for (let i = newArray.length - 1; i > 0; i--) {
        const j = Math.floor(Math.random() * (i + 1));
        [newArray[i], newArray[j]] = [newArray[j], newArray[i]];
    }
    return newArray;
}

export async function runFetchPipeline(specificSourceId?: string) {
    const batchId = crypto.randomUUID();
    const startedAt = new Date().toISOString();
    let logId: string | null = null;

    // Initialize stats
    const statsDetail = {
        total_scraped: 0,
        skipped_duplicate: 0,
        ai_processed: 0,
        ai_skipped: 0,
        ai_failed: 0,
        published_count: 0,
        reasons: {} as Record<string, number>
    };

    const addReason = (reason: string) => {
        statsDetail.reasons[reason] = (statsDetail.reasons[reason] || 0) + 1;
    };

    try {
        // Create initial log entry
        const { data: logData, error: logError } = await supabaseAdmin
            .from('fetch_logs')
            .insert([{
                batch_id: batchId,
                started_at: startedAt,
                status: 'running'
            }])
            .select()
            .single();

        if (logData) logId = logData.id;

        const sources = await getActiveNewsSources();
        const targetSources = specificSourceId
            ? sources.filter(s => s.id === specificSourceId)
            : sources;

        // 0. Pre-fetch categories for mapping
        const { data: categoriesData } = await supabaseAdmin.from('categories').select('id, name');
        const categoryMap: Record<string, string> = {};
        categoriesData?.forEach(c => categoryMap[c.name] = c.id);

        const batchTime = new Date().toISOString();

        // Initialize status
        await updateFetchStatus({
            is_running: true,
            current_source: 'é˜¶æ®µ 1: æ­£åœ¨ä»å„æºæŠ“å–åŸå§‹æ•°æ®...',
            progress: 0,
            total: targetSources.length,
            started_at: batchTime
        });

        console.log(`ğŸš€ å¼€å§‹æŠ“å–é˜¶æ®µ: å…± ${targetSources.length} ä¸ªæº`);

        // --- STAGE 1: å¿«é€ŸæŠ“å–å¹¶å­˜ä¸ºè‰ç¨¿ ---
        for (let i = 0; i < targetSources.length; i++) {
            const source = targetSources[i];
            await updateFetchStatus({ is_running: true, current_source: `æŠ“å–ä¸­: ${source.name}`, progress: i, total: targetSources.length });

            try {
                const scrapedItems = await scrapeContent(source.url, source.source_type, source.youtube_channel_id);
                statsDetail.total_scraped += scrapedItems.length;

                for (const item of scrapedItems) {
                    // 1. ä¸¥æ ¼æ£€æŸ¥ URL æ˜¯å¦å®Œå…¨é‡å¤ï¼ˆå…¨å±€ï¼‰
                    const { data: exactUrlMatch } = await supabaseAdmin
                        .from('news_items')
                        .select('id')
                        .eq('original_url', item.url)
                        .maybeSingle();

                    if (exactUrlMatch) {
                        statsDetail.skipped_duplicate++;
                        addReason('Duplicate (Global URL Match)');
                        continue;
                    }

                    // 2. æ£€æŸ¥æ ‡é¢˜/å†…å®¹çš„ç›¸ä¼¼åº¦ï¼ˆ48å°æ—¶çª—å£ï¼‰
                    const { data: exists } = await supabaseAdmin.rpc('find_similar_news', {
                        check_title: item.title,
                        check_url: item.url,
                        time_window_hours: 48,
                        similarity_threshold: 0.8
                    });

                    if (exists && exists.length > 0) {
                        statsDetail.skipped_duplicate++;
                        addReason('Duplicate (Similarity Check)');
                        continue;
                    }

                    // å­˜ä¸ºè‰ç¨¿
                    const { error: insertError } = await supabaseAdmin.from('news_items').insert([{
                        source_id: source.id,
                        original_url: item.url,
                        title: item.title,
                        content: item.content,
                        content_type: item.contentType,
                        published_at: item.publishedAt?.toISOString(),
                        video_id: item.videoId,
                        image_url: item.imageUrl,
                        fetch_batch_id: batchId,
                        is_published: false,
                    }]);

                    if (insertError) {
                        // å¦‚æœä¾ç„¶æŠ¥å”¯ä¸€æ€§å†²çªé”™è¯¯ï¼ŒæŒ‰é‡å¤å¤„ç†
                        if (insertError.code === '23505') {
                            statsDetail.skipped_duplicate++;
                            addReason('Duplicate (Insert Race Condition)');
                        } else {
                            statsDetail.ai_failed++;
                            addReason(`Insert Error: ${insertError.message}`);
                        }
                    }
                }

                await updateLastFetchedTime(source.id);
            } catch (err) {
                console.error(`Failed to scrape ${source.name}:`, err);
                addReason(`Scrape Failed (${source.name})`);
            }
        }

        console.log(`âœ… é˜¶æ®µ 1 å®Œæˆ: æŠ“å–åˆ° ${statsDetail.total_scraped} æ¡å†…å®¹ã€‚å‡†å¤‡è¿›å…¥æµç¨‹å¤„ç†...`);

        // --- STAGE 2: ä¼˜å…ˆå¤„ç†ç§¯å‹çš„æ—§è‰ç¨¿ ---
        const { data: drafts } = await supabaseAdmin
            .from('news_items')
            .select('*, source:news_sources(commentary_style)')
            .eq('is_published', false)
            .order('created_at', { ascending: true })
            .limit(200);

        if (!drafts || drafts.length === 0) {
            await updateFetchStatus({ is_running: false, current_source: 'æ— æ–°å†…å®¹éœ€å¤„ç†', progress: targetSources.length, total: targetSources.length });

            if (logId) {
                await supabaseAdmin.from('fetch_logs').update({
                    status: 'completed',
                    completed_at: new Date().toISOString(),
                    total_scraped: statsDetail.total_scraped,
                    skipped_duplicate: statsDetail.skipped_duplicate,
                    published_count: 0
                }).eq('id', logId);
            }

            return { success: true, newItems: 0 };
        }

        console.log(`ğŸ§  å¼€å§‹ AI å¤„ç†é˜¶æ®µ: å¾…å¤„ç† ${drafts.length} æ¡æ–°é—»`);
        statsDetail.ai_processed = drafts.length;

        for (let i = 0; i < drafts.length; i++) {
            const news = drafts[i];

            // ğŸ›‘ æ¯å¤„ç† 5 æ¡æ–°é—»æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰äººæ‰‹åŠ¨ç‚¹å‡»äº†â€œé‡ç½®â€
            if (i % 5 === 0) {
                const { data: currentStatus } = await supabaseAdmin
                    .from('system_settings')
                    .select('value')
                    .eq('key', 'fetch_status')
                    .single();

                if (currentStatus?.value?.is_running === false) {
                    console.log('ğŸ›‘ ç”¨æˆ·å·²è¯·æ±‚é‡ç½®ï¼Œæ­£åœ¨ä¸­æ­¢æŠ“å–æµæ°´çº¿...');
                    return { success: false, message: 'æµæ°´çº¿è¢«ç”¨æˆ·ä¸­æ­¢' };
                }
            }

            await updateFetchStatus({
                is_running: true,
                current_source: `AI åˆ†æä¸­ (${i + 1}/${drafts.length}): ${news.title.substring(0, 20)}...`,
                progress: i,
                total: drafts.length
            });

            try {
                const analysis = await analyzeContent(
                    news.content,
                    news.title,
                    news.source?.commentary_style || '',
                    news.content_type || 'article'
                );

                if (analysis.shouldSkip) {
                    statsDetail.ai_skipped++;
                    addReason(`AI Filtered: ${analysis.skipReason || 'Quality'}`);
                    await supabaseAdmin.from('news_items').delete().eq('id', news.id);
                    continue;
                }

                let categoryName = analysis.category || 'çƒ­ç‚¹';
                if (categoryName.includes('æœ¬åœ°')) categoryName = 'æœ¬åœ°';
                else if (categoryName.includes('çƒ­ç‚¹')) categoryName = 'çƒ­ç‚¹';
                else if (categoryName.includes('ç§‘æŠ€')) categoryName = 'ç§‘æŠ€';
                else if (categoryName.includes('è´¢ç»')) categoryName = 'è´¢ç»';
                else if (categoryName.includes('æ·±åº¦')) categoryName = 'æ·±åº¦';

                const catId = categoryMap[categoryName] || categoryMap['çƒ­ç‚¹'] || Object.values(categoryMap)[0];

                const updatePayload: any = {
                    title: analysis.translatedTitle || news.title,
                    ai_summary: analysis.summary,
                    ai_commentary: analysis.commentary,
                    category_id: catId,
                    tags: Array.isArray(analysis.tags) ? analysis.tags : [],
                    location: analysis.location, // å°è¯•åŒ…å«ä½ç½®ä¿¡æ¯
                    is_published: true,
                    batch_completed_at: batchTime,
                    updated_at: new Date().toISOString()
                };

                let { error: updateError } = await supabaseAdmin.from('news_items').update(updatePayload).eq('id', news.id);

                // ğŸ›‘ å¦‚æœæŠ¥é”™â€œæ‰¾ä¸åˆ° location åˆ—â€ï¼Œåˆ™å‰”é™¤è¯¥åˆ—é‡æ–°å°è¯•
                if (updateError && (updateError.message.includes('column "location" of relation "news_items" does not exist') || updateError.message.includes('Could not find the \'location\' column'))) {
                    console.warn('âš ï¸ æ•°æ®åº“ç¼ºå°‘ location åˆ—ï¼Œæ­£åœ¨å‰”é™¤è¯¥åˆ—é‡è¯•...');
                    delete updatePayload.location;
                    const { error: retryError } = await supabaseAdmin.from('news_items').update(updatePayload).eq('id', news.id);
                    updateError = retryError;
                }

                if (updateError) {
                    statsDetail.ai_failed++;
                    addReason(`DB Update Error: ${updateError.message}`);
                } else {
                    statsDetail.published_count++;
                }

                await new Promise(r => setTimeout(r, 500));
            } catch (err) {
                console.error(`AI é˜¶æ®µå¤„ç†å¤±è´¥ [${news.id}]:`, err);
                statsDetail.ai_failed++;
                addReason(`AI Process Crash: ${String(err)}`);
            }
        }

        // --- FINAL LOG UPDATE ---
        if (logId) {
            await supabaseAdmin.from('fetch_logs').update({
                status: 'completed',
                completed_at: new Date().toISOString(),
                total_scraped: statsDetail.total_scraped,
                skipped_duplicate: statsDetail.skipped_duplicate,
                ai_processed: statsDetail.ai_processed,
                ai_skipped: statsDetail.ai_skipped,
                ai_failed: statsDetail.ai_failed,
                published_count: statsDetail.published_count,
                failure_reasons: statsDetail.reasons
            }).eq('id', logId);
        }

        await updateFetchStatus({
            is_running: false,
            current_source: `å¤„ç†å®Œæˆ: æ–°å‘å¸ƒ ${statsDetail.published_count} æ¡`,
            progress: drafts.length,
            total: drafts.length,
            last_completed_at: new Date().toISOString()
        });

        return { success: true, newItems: statsDetail.published_count, stats: statsDetail };

    } catch (error) {
        console.error('Error in fetch pipeline:', error);
        await updateFetchStatus({ is_running: false, error: String(error) });

        if (logId) {
            await supabaseAdmin.from('fetch_logs').update({
                status: 'failed',
                completed_at: new Date().toISOString(),
                failure_reasons: { ...statsDetail.reasons, critical_error: String(error) }
            }).eq('id', logId);
        }

        throw error;
    }
}
